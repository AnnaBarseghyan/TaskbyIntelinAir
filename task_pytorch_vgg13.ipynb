{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5b6qlD2QFTE",
    "outputId": "22227e33-7ec8-4d2b-c2f4-4c2e18a326bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 1.18.0 not found\n",
      "Requirement already satisfied: pandas==1.1.0 in ./opt/anaconda3/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas==1.1.0) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas==1.1.0) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas==1.1.0) (1.19.2)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.1.0) (1.15.0)\n",
      "Requirement already satisfied: opencv-python==4.4.0.46 in ./opt/anaconda3/lib/python3.8/site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.17.3 in ./opt/anaconda3/lib/python3.8/site-packages (from opencv-python==4.4.0.46) (1.19.2)\n",
      "zsh:1: no matches found: rasterio[s3]==1.1.8\n",
      "Requirement already satisfied: awscli in ./opt/anaconda3/lib/python3.8/site-packages (1.19.57)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in ./opt/anaconda3/lib/python3.8/site-packages (from awscli) (0.4.2)\n",
      "Requirement already satisfied: PyYAML<5.5,>=3.10 in ./opt/anaconda3/lib/python3.8/site-packages (from awscli) (5.3.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in ./opt/anaconda3/lib/python3.8/site-packages (from awscli) (0.15.2)\n",
      "Requirement already satisfied: botocore==1.20.57 in ./opt/anaconda3/lib/python3.8/site-packages (from awscli) (1.20.57)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2; python_version > \"2.7\" in ./opt/anaconda3/lib/python3.8/site-packages (from awscli) (4.6)\n",
      "Requirement already satisfied: colorama<0.4.4,>=0.2.5 in ./opt/anaconda3/lib/python3.8/site-packages (from awscli) (0.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore==1.20.57->awscli) (1.25.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore==1.20.57->awscli) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore==1.20.57->awscli) (2.8.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./opt/anaconda3/lib/python3.8/site-packages (from rsa<4.8,>=3.1.2; python_version > \"2.7\"->awscli) (0.4.8)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.20.57->awscli) (1.15.0)\n",
      "Requirement already satisfied: geopandas in ./opt/anaconda3/lib/python3.8/site-packages (0.9.0)\n",
      "Requirement already satisfied: shapely>=1.6 in ./opt/anaconda3/lib/python3.8/site-packages (from geopandas) (1.7.1)\n",
      "Requirement already satisfied: fiona>=1.8 in ./opt/anaconda3/lib/python3.8/site-packages (from geopandas) (1.8.19)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in ./opt/anaconda3/lib/python3.8/site-packages (from geopandas) (3.0.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in ./opt/anaconda3/lib/python3.8/site-packages (from geopandas) (1.1.0)\n",
      "Requirement already satisfied: six>=1.7 in ./opt/anaconda3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
      "Requirement already satisfied: cligj>=0.5 in ./opt/anaconda3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (0.7.1)\n",
      "Requirement already satisfied: click<8,>=4.0 in ./opt/anaconda3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
      "Requirement already satisfied: click-plugins>=1.0 in ./opt/anaconda3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: attrs>=17 in ./opt/anaconda3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (20.3.0)\n",
      "Requirement already satisfied: certifi in ./opt/anaconda3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (2020.6.20)\n",
      "Requirement already satisfied: munch in ./opt/anaconda3/lib/python3.8/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: numpy>=1.15.4 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.0->geopandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.0->geopandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in ./opt/anaconda3/lib/python3.8/site-packages (from pandas>=0.24.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: boto3 in ./opt/anaconda3/lib/python3.8/site-packages (1.17.57)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in ./opt/anaconda3/lib/python3.8/site-packages (from boto3) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.57 in ./opt/anaconda3/lib/python3.8/site-packages (from boto3) (1.20.57)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./opt/anaconda3/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.57->boto3) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./opt/anaconda3/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.57->boto3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in ./opt/anaconda3/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.57->boto3) (1.15.0)\n",
      "Requirement already satisfied: rasterio in ./opt/anaconda3/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: affine in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (2.3.0)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (1.19.2)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (1.4.7)\n",
      "Requirement already satisfied: attrs in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (20.3.0)\n",
      "Requirement already satisfied: cligj>=0.5 in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (0.7.1)\n",
      "Requirement already satisfied: certifi in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (2020.6.20)\n",
      "Requirement already satisfied: click-plugins in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (1.1.1)\n",
      "Requirement already satisfied: click<8,>=4.0 in ./opt/anaconda3/lib/python3.8/site-packages (from rasterio) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.6 in ./opt/anaconda3/lib/python3.8/site-packages (from snuggs>=1.4.1->rasterio) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy>=1.18.0\n",
    "! pip install pandas==1.1.0\n",
    "! pip install opencv-python==4.4.0.46\n",
    "! pip install rasterio[s3]==1.1.8\n",
    "! pip install awscli\n",
    "! pip install geopandas\n",
    "!pip install boto3\n",
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NETKse6vd1KQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_NO_SIGN_REQUEST'] = 'YES'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IounsGxKQno3"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "import math\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from typing import Tuple, List, Sequence\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import botocore\n",
    "import cv2\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.io import DatasetReader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLi7cDup6su1"
   },
   "source": [
    "## Utility Functions & Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-SVMSTSy6wL4"
   },
   "outputs": [],
   "source": [
    "def read_image(image_path: str or Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reads an image and returns it.\n",
    "    Args:\n",
    "        image_path: the path of the image\n",
    "    Returns:\n",
    "        array of image\n",
    "    \"\"\"\n",
    "    image_path = image_path if isinstance(image_path, str) else str(image_path)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "    if image is None:\n",
    "        raise ValueError(f'Failed to read image from {image_path} path.')\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "gQW2snPc6wZN"
   },
   "outputs": [],
   "source": [
    "def save_image(image: np.ndarray, image_path: str or Path, image_band: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Saves image and returns True in case of successful saving.\n",
    "    Args:\n",
    "        image: the image array to save\n",
    "        image_path: the path of the image\n",
    "        image_band: the image band name, which will be used in case of various color conversions\n",
    "    Returns:\n",
    "        image_path if image is successfully saved\n",
    "    Raises:\n",
    "        ValueError if the image is not saved\n",
    "    \"\"\"\n",
    "    image_path = image_path if isinstance(image_path, str) else str(image_path)\n",
    "\n",
    "    if image_band == 'RGB':\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    try:\n",
    "        check = cv2.imwrite(image_path, image)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f'Failed to save image in {image_path} path due to error:\\n {e}.')\n",
    "    else:\n",
    "        if check:\n",
    "            return image_path\n",
    "        else:\n",
    "            ValueError(f'Failed to save image in {image_path} path.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CHd3UHf46wep"
   },
   "outputs": [],
   "source": [
    "def crop_image(image: np.ndarray, crop_area: Tuple[int, int, int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Crops an are of image using its (top, left, bottom, right) coordinates.\n",
    "    Args:\n",
    "        image: 2D or 3D image, with shapes (H, W) or (H, W, C)\n",
    "        crop_area: (top, left, bottom, right) coordinates of area\n",
    "\n",
    "    Returns:\n",
    "        cropped image array\n",
    "    \"\"\"\n",
    "    top, left, bottom, right = crop_area\n",
    "    if image.ndim == 2:\n",
    "        return image[top:bottom, left:right]\n",
    "    elif image.ndim == 3:\n",
    "        return image[top:bottom, left:right, :]\n",
    "    else:\n",
    "        raise ValueError(f'The input image must be 2D or 3D, got: {image.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PxUiWbRY6whx"
   },
   "outputs": [],
   "source": [
    "def resize_image(image: np.ndarray, size: Tuple[int, int], algorithm=cv2.INTER_CUBIC):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: the image to resize\n",
    "        size: the size of the new image - (H, W)\n",
    "        algorithm: the algorithm used for resizing the image\n",
    "\n",
    "                    Possible algorithms are:\n",
    "                      INTER_NEAREST      = 0 - nearest neighbor interpolation\n",
    "                      INTER_LINEAR       = 1 - bilinear interpolation\n",
    "                      INTER_CUBIC        = 2 - bicubic interpolation\n",
    "                      INTER_AREA         = 3 - resampling using pixel area relation\n",
    "                      INTER_LANCZOS4     = 4 - Lanczos interpolation over 8x8 neighborhood\n",
    "                      INTER_LINEAR_EXACT = 5 - Bit exact bilinear interpolation\n",
    "    Returns:\n",
    "        resized image array\n",
    "    \"\"\"\n",
    "    # if size is None or equal to current size then return the original image\n",
    "    if size is None or size == image.shape[:2]:\n",
    "        return image\n",
    "\n",
    "    # openCV takes fx=W and fy=H\n",
    "    resized = cv2.resize(image, tuple(reversed(size)), interpolation=algorithm)\n",
    "\n",
    "    # openCV losses single channel images 3rd dimension\n",
    "    if image.ndim == 3 and resized.ndim != 3:\n",
    "        resized = np.expand_dims(resized, axis=2)\n",
    "\n",
    "    return resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "5GA9hURB6wlP"
   },
   "outputs": [],
   "source": [
    "class Slider:\n",
    "    \"\"\"Special class for sliding images, aka creatint tiles or patches.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 window: int or Tuple[int, int],\n",
    "                 stride: int or Tuple[int, int],\n",
    "                 scales: float or Iterable[float] = (1.0,), ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            window: int or (int, int), (h, w) the window size,\n",
    "            stride: int or (int, int), (dh, dw) the strides,\n",
    "            scales: (float ...), the scales to resize the original image\n",
    "                    <1: downscale the original image\n",
    "                    >1: upscale\n",
    "        \"\"\"\n",
    "        if isinstance(window, int):\n",
    "            self.window = (window, window)\n",
    "        elif isinstance(window, tuple):\n",
    "            self.window = window\n",
    "        else:\n",
    "            raise TypeError(f'window must be tuple or int, got {type(window)}')\n",
    "\n",
    "        if isinstance(stride, int):\n",
    "            self.stride = (stride, stride)\n",
    "        elif isinstance(stride, tuple):\n",
    "            self.stride = stride\n",
    "        else:\n",
    "            raise TypeError(f'stride must be tuple or int, got: {type(stride)}')\n",
    "\n",
    "        if isinstance(scales, (float, int)):\n",
    "            self.scales = (scales,)\n",
    "        elif isinstance(scales, Iterable):\n",
    "            self.scales = scales\n",
    "        else:\n",
    "            raise TypeError(f'scales must be iterable or float or int, got: {type(scales)}')\n",
    "\n",
    "    def get_number_of_patches(self, input_h: int, input_w: int) -> int:\n",
    "        \"\"\"For given input sizes returns the possible number of sliding operations or the number of patches.\"\"\"\n",
    "        n_vertical = math.ceil((input_h - self.window[0]) / self.stride[0]) + 1\n",
    "        n_horizontal = math.ceil((input_w - self.window[1]) / self.stride[1]) + 1\n",
    "        n_scales = sum(1 for _ in self.scales)\n",
    "        return n_horizontal * n_vertical * n_scales\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_scale(scale: float, h: int, w: int) -> Tuple[int, int]:\n",
    "        \"\"\"Applies scaling.\"\"\"\n",
    "        return int(h / scale), int(w / scale)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_1d_offset(current_position: int, step: int, original_size: int) -> Tuple[int, int]:\n",
    "        \"\"\"Returns the 1d offset coordinates as a result of one step from given position.\n",
    "        If the potential window size exceed the original image size for instance,\n",
    "        window_h > image_h, window_h will thrink to image_h. \"\"\"\n",
    "        if current_position == 0 and step > original_size:\n",
    "            offset = (0, original_size)\n",
    "        elif current_position + step > original_size:\n",
    "            offset = (original_size - step, original_size)\n",
    "        else:\n",
    "            offset = (current_position, current_position + step)\n",
    "        return offset\n",
    "\n",
    "    def get_offsets(self, input_h: int, input_w: int) -> List[Tuple[int, int, int, int]]:\n",
    "        \"\"\"\n",
    "        Returns the offsets: (top, left, bottom, right) to get sub-images as a result of sliding.\n",
    "        Args:\n",
    "            input_h: input image height\n",
    "            input_w: input image width\n",
    "        Returns:\n",
    "            list of tuples where each tuple contains the patches or sub-images coordinates: (top, left, bottom, right)\n",
    "        \"\"\"\n",
    "        all_offsets = []\n",
    "        for s in self.scales:\n",
    "            window_h, window_w = self.apply_scale(s, *self.window)\n",
    "            stride_h, stride_w = self.apply_scale(s, *self.stride)\n",
    "            hs = [self.get_1d_offset(h, window_h, input_h) for h in range(0, input_h - window_h + stride_h, stride_h)]\n",
    "            ws = [self.get_1d_offset(w, window_w, input_w) for w in range(0, input_w - window_w + stride_w, stride_w)]\n",
    "            all_offsets += list(itertools.product(hs, ws))\n",
    "\n",
    "        return [(x[0][0], x[1][0], x[0][1], x[1][1]) for x in all_offsets]\n",
    "\n",
    "    def get_patch(self, image: np.ndarray, offset: Tuple[int, int, int, int]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: original image to extract patch, which must have the shape H x W x C\n",
    "            offset: the tuples(top, left, bottom, right)\n",
    "        Returns:\n",
    "            image patch or sub-image window_h x window_h x C\n",
    "        \"\"\"\n",
    "        return resize_image(crop_image(image, offset), self.window, algorithm=cv2.INTER_CUBIC)\n",
    "\n",
    "    def get_patches(self, image: np.ndarray, offsets: List[Tuple[int, int, int, int]]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image: original image to extract patches, which must have the shape H x W x C\n",
    "            offsets: the list of tuples (top, left, bottom, right)\n",
    "        Returns:\n",
    "            image patches concatenated in one array with shape: N x window_h x window_h x C,\n",
    "            where N = len(offsets)\n",
    "        \"\"\"\n",
    "        return np.stack(tuple(self.get_patch(image, offset) for offset in offsets))\n",
    "\n",
    "    @staticmethod\n",
    "    def reconstruct(patches: Sequence[np.ndarray],\n",
    "                    offsets: List[Tuple[int, int, int, int]],\n",
    "                    output_shape: Tuple[int, int, int] or Tuple[int, int]):\n",
    "        \"\"\"\n",
    "        Recovers the image using its patches and their coordinates. Assumes that patches may\n",
    "        overlap. Overlapped patches are averaged.\n",
    "        Args:\n",
    "            patches: the list of image patches\n",
    "            offsets: the list of offsets (patch coordinates (top, left, bottom, right))\n",
    "            output_shape: the shape of the output image (H, W, C) or (H, W)\n",
    "        Returns:\n",
    "            the array of output image with shape = output shape\n",
    "        \"\"\"\n",
    "\n",
    "        if len(patches) != len(offsets):\n",
    "            raise ValueError('Number of patches must be equal to number of offsets.')\n",
    "\n",
    "        out_image, counter = np.zeros(output_shape), np.zeros(output_shape, dtype=np.float32)\n",
    "        for offset, patch in zip(offsets, patches):\n",
    "            top, left, bottom, right = offset\n",
    "            out_image[top:bottom, left:right] += patch\n",
    "            counter[top:bottom, left:right] += 1.\n",
    "\n",
    "        out_image /= counter\n",
    "        return out_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bkXSUj0RaA9"
   },
   "source": [
    "## Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4VcL7Y7L7acz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FlightObject:\n",
    "    \"\"\"Special class for preparing data using flight id and field id.\"\"\"\n",
    "\n",
    "    BANDS = ('rgb', 'mask')\n",
    "\n",
    "    def __init__(self, flight_id: str, field_id: str, download_folder: str):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            flight_id: the unique id of the flight\n",
    "            field_id: the unique id of the field\n",
    "            download_folder: the folder path where the flight data is downloaded\n",
    "        \"\"\"\n",
    "        paths = self.get_image_paths_from_flight_code(flight_id, download_folder)\n",
    "        self._download_folder = download_folder\n",
    "        self._border_file_path = paths.pop('border_file_path')\n",
    "        self._image_paths = {k.replace('_path', ''): v for k, v in paths.items()}\n",
    "\n",
    "        self._flight_id = flight_id\n",
    "        self._field_id = field_id\n",
    "        self._output_depth = 'uint16'\n",
    "\n",
    "        self._base_name = f\"{self._field_id}_{self._flight_id}\"\n",
    "        self._crop_coordinates = None\n",
    "        self._mask = None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image_paths_from_flight_code(flight_code: str, download_folder: str) -> Dict[str, str]:\n",
    "        \"\"\"Returns r, g, b and boarder file paths based on flight code.\"\"\"\n",
    "        return {\n",
    "            'red_path': f'{download_folder}/{flight_code}/reflectance_red-red.tif',\n",
    "            'green_path': f'{download_folder}/{flight_code}/reflectance_green-green.tif',\n",
    "            'blue_path': f'{download_folder}/{flight_code}/reflectance_blue-blue.tif',\n",
    "            'border_file_path': f'{download_folder}/{flight_code}/boundary.zip'\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def image_paths(self) -> Dict[str, str]:\n",
    "        \"\"\"Returns images paths in S3.\"\"\"\n",
    "        return self._image_paths.copy()\n",
    "\n",
    "    @property\n",
    "    def border_file_path(self) -> str:\n",
    "        \"\"\"Returns images paths in S3.\"\"\"\n",
    "        return self._border_file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def read_single_channel_raster_image(image_path: str,\n",
    "                                         get_dataset: bool, ) -> np.ndarray or Tuple[np.ndarray, DatasetReader]:\n",
    "        \"\"\"Reads single channel raster image\"\"\"\n",
    "        with rasterio.open(image_path) as src:\n",
    "            image = np.nan_to_num(np.squeeze(src.read()), nan=0, posinf=0, neginf=0).astype(np.uint16)\n",
    "        return (image, src) if get_dataset else image\n",
    "\n",
    "    @staticmethod\n",
    "    def get_border_mask(border_file_path, raster_dataset: DatasetReader) -> np.ndarray:\n",
    "        \"\"\"Returns boolean mask, where True means that pixel belongs to the field, aka inside the border geometry.\"\"\"\n",
    "        gdf = gpd.read_file(border_file_path)\n",
    "        if gdf.shape[0] != 1 or 'geometry' not in gdf.columns:\n",
    "            raise ValueError(f'Boundary file {border_file_path} is wrong.')\n",
    "\n",
    "        border_geometry = gdf.to_crs(crs=raster_dataset.crs)['geometry'].iloc[0]\n",
    "        mask, _, _ = rasterio.mask.raster_geometry_mask(raster_dataset, [border_geometry], crop=False, invert=True)\n",
    "        return mask\n",
    "\n",
    "    def get_mask(self) -> np.ndarray:\n",
    "        \"\"\"Returns boolean Border Mask.\"\"\"\n",
    "        red_image, red_raster_dataset = self.read_single_channel_raster_image(\n",
    "            self._image_paths['red'], True\n",
    "        )\n",
    "\n",
    "        border_mask = self.get_border_mask(self._border_file_path, red_raster_dataset)\n",
    "        border_mask = np.nan_to_num(border_mask, nan=0, posinf=0, neginf=0)\n",
    "\n",
    "        x_index, y_index = np.nonzero(border_mask)\n",
    "        self._crop_coordinates = (x_index.min(), y_index.min(), x_index.max(), y_index.max())\n",
    "        self._mask = crop_image(border_mask, self._crop_coordinates).astype(np.uint8)\n",
    "\n",
    "        return self._mask.copy()\n",
    "\n",
    "    def get_rgb(self) -> np.ndarray:\n",
    "        \"\"\"Returns RGB image as uint8 or unit16.\"\"\"\n",
    "        rgb_image = np.stack(\n",
    "            [\n",
    "                self.read_single_channel_raster_image(self._image_paths[c], False)\n",
    "                for c in ('red', 'green', 'blue')\n",
    "            ],\n",
    "            axis=2\n",
    "        )\n",
    "        return crop_image(rgb_image, self._crop_coordinates)\n",
    "\n",
    "    def get_images(self) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Returns all images.\"\"\"\n",
    "        mask = self.get_mask()\n",
    "        rgb = self.get_rgb()\n",
    "        rgb *= mask[..., np.newaxis]\n",
    "        return {'rgb': rgb, 'mask': mask}\n",
    "\n",
    "    def create_tiles(self, output_folder: Path, slider_config: Dict) -> List[str]:\n",
    "        \"\"\"\n",
    "        Scales images by given scale factor, creates patches and saves them in output folder.\n",
    "        Args:\n",
    "            output_folder: the folder where the patched images will be saved\n",
    "            slider_config: the config creating patches (tiles)\n",
    "        \"\"\"\n",
    "        factor = 2**14\n",
    "        rgb = self.get_images()['rgb']\n",
    "        rgb = np.clip((rgb / factor) * 255,0, 255)\n",
    "        slider = Slider(**slider_config)\n",
    "        offsets = slider.get_offsets(*rgb.shape[:2])\n",
    "\n",
    "        tile_paths = []\n",
    "        for i, offset in enumerate(offsets):\n",
    "            tile = slider.get_patch(rgb, offset)\n",
    "            band_image_name = f'{self._base_name}_rgb_{i}-patch.png'\n",
    "            check = np.all(tile == 0)\n",
    "            if check:\n",
    "                print(f'{self._flight_id}-{self._field_id}: patch_number={i} have been skipped.')\n",
    "            else:\n",
    "                output_path = output_folder.joinpath(band_image_name)\n",
    "                output_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "                tile_paths.append(save_image(tile, output_path, image_band='rgb'))\n",
    "        print(f'{self._base_name}: n={len(tile_paths)} patches have been saved, original size={rgb.shape}.')\n",
    "        return tile_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "FAPiak9C7ajS"
   },
   "outputs": [],
   "source": [
    "class DatasetCreator:\n",
    "    \"\"\"Special Class for downloading images from S3 and splitting them into small tiles.\"\"\"\n",
    "\n",
    "    def __init__(self, s3_uri: str, download_folder: str, output_folder: str, tile_size: int = 512):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            s3_uri: the  S3 URI - path of the images in S3\n",
    "            download_folder: the folder to store raw images\n",
    "            output_folder: the folder to store tiles (aka, small images with equal sizes)\n",
    "            tile_size: the size of tiles, default: 512\n",
    "        \"\"\"\n",
    "\n",
    "        self.download_folder = Path(download_folder)\n",
    "        self.output_folder = Path(output_folder)\n",
    "\n",
    "        self.download_folder.mkdir(parents=True, exist_ok=True)\n",
    "        self.output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        parsed_uri = urlparse(s3_uri, allow_fragments=False)\n",
    "        self.s3_uri = s3_uri\n",
    "        self.s3_bucket = parsed_uri.netloc\n",
    "        self.s3_folder = parsed_uri.path.lstrip('/')\n",
    "\n",
    "        self.tile_size = tile_size\n",
    "\n",
    "    @staticmethod\n",
    "    def download_s3_dir(bucket: str, folder_path: str, download_folder: Path):\n",
    "        \"\"\"Downloads s3 directory to download folder.\"\"\"\n",
    "        client = boto3.client('s3')\n",
    "        client.meta.events.register('choose-signer.s3.*', botocore.handlers.disable_signing)\n",
    "\n",
    "        # Handle missing / at end of prefix\n",
    "        if not folder_path.endswith('/'):\n",
    "            folder_path += '/'\n",
    "\n",
    "        paginator = client.get_paginator('list_objects_v2')\n",
    "        for result in paginator.paginate(Bucket=bucket, Prefix=folder_path):\n",
    "            # Download each file individually\n",
    "            for key in tqdm(result['Contents'], total=len(result['Contents'])):\n",
    "                # Calculate relative path\n",
    "                rel_path = key['Key'][len(folder_path):]\n",
    "                # Skip paths ending in /\n",
    "                if not key['Key'].endswith('/'):\n",
    "                    local_file_path = download_folder.joinpath(rel_path)\n",
    "                    local_file_path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "                    # Skip download if already downloaded\n",
    "                    if not local_file_path.exists():\n",
    "                        client.download_file(bucket, key['Key'], str(local_file_path))\n",
    "\n",
    "    def download_data(self):\n",
    "        \"\"\"Downloads data from S3.\"\"\"\n",
    "        self.download_s3_dir(self.s3_bucket, self.s3_folder, self.download_folder)\n",
    "        df = pd.read_csv(self.download_folder.joinpath('final_data.CSV'))\n",
    "        df['paths'] = df['paths'].apply(ast.literal_eval)\n",
    "        df['local_paths'] = df['paths'].apply(\n",
    "            lambda x: {k: v.replace(self.s3_uri, str(self.download_folder)) for k, v in x.items()}\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def create_tiles(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Creates tiles from downloaded images.\"\"\"\n",
    "        slider_config = {'window': self.tile_size, 'stride': self.tile_size, 'scales': (1,)}\n",
    "\n",
    "        final_df = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            flight_obj = FlightObject(row['flight_code'], row['field_token'], self.download_folder)\n",
    "            tiles_paths = flight_obj.create_tiles(self.output_folder, slider_config)\n",
    "            for i, tile_path in enumerate(tiles_paths):\n",
    "                final_df.append(\n",
    "                    {\n",
    "                        'flight_code': row['flight_code'],\n",
    "                        'field_token': row['field_token'],\n",
    "                        'class': row['ActualStatus'],\n",
    "                        'flight_date': row['flight_date'],\n",
    "                        'flight_number': row['flight_number'],\n",
    "                        'tile_number': i,\n",
    "                        'image_path': tile_path,\n",
    "                    }\n",
    "                )\n",
    "        final_df = pd.DataFrame(final_df)\n",
    "        final_df['class'] = final_df['class'].isin(('Planted', 'Emerged'))\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"Downloads data, creates tiles and saves them in output folder.\"\"\"\n",
    "        df = self.download_data()\n",
    "        tiles_df = self.create_tiles(df)\n",
    "        tiles_df.to_csv(self.output_folder.joinpath('tiles.CSV'), index=False)\n",
    "        return tiles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "SVfKtSYh7amX"
   },
   "outputs": [],
   "source": [
    "S3_URI = 's3://intelinair-internship/field_state_classification/'\n",
    "DOWNLOAD_FOLDER = 'anna/image_data'\n",
    "OUTPUT_FOLDER = 'anna/final_image_data'\n",
    "TILE_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "Qih5aIwD7aqS"
   },
   "outputs": [],
   "source": [
    "creator = DatasetCreator(S3_URI, DOWNLOAD_FOLDER, OUTPUT_FOLDER, tile_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "em3QNf1Y7as5",
    "outputId": "eb03bfb0-033b-4d10-c62a-dbcdba362690"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [00:00<00:00, 2859.21it/s]\n",
      "  3%|▎         | 1/30 [00:06<02:55,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_1TJZT39WQ: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:17<03:34,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2HHWV1DD4-7e227290-9936-483e-90de-949c80f984d7: patch_number=116 have been skipped.\n",
      "2HHWV1DD4-7e227290-9936-483e-90de-949c80f984d7: patch_number=117 have been skipped.\n",
      "2HHWV1DD4-7e227290-9936-483e-90de-949c80f984d7: patch_number=118 have been skipped.\n",
      "2HHWV1DD4-7e227290-9936-483e-90de-949c80f984d7: patch_number=119 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_2HHWV1DD4: n=116 patches have been saved, original size=(10086, 2669, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:23<03:15,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3KR1212BR-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "3KR1212BR-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "3KR1212BR-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_3KR1212BR: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:29<02:58,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42T73AQE8-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "42T73AQE8-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "42T73AQE8-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_42T73AQE8: n=77 patches have been saved, original size=(7980, 2116, 3).\n",
      "E83UE8JT4-b7add13d-8802-46cb-8d3f-cb1ce7daf029: patch_number=0 have been skipped.\n",
      "E83UE8JT4-b7add13d-8802-46cb-8d3f-cb1ce7daf029: patch_number=8 have been skipped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:36<02:52,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E83UE8JT4-b7add13d-8802-46cb-8d3f-cb1ce7daf029: patch_number=81 have been skipped.\n",
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_E83UE8JT4: n=87 patches have been saved, original size=(4708, 4541, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:42<02:39,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EB4KG2F6D-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "EB4KG2F6D-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "EB4KG2F6D-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_EB4KG2F6D: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:48<02:28,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_EVTQTDFEJ: n=81 patches have been saved, original size=(4451, 4291, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:53<02:13,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_F98U2YA3K: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:59<02:01,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_FPG2L7UTK: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [01:04<01:56,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVI8IQG1F-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "FVI8IQG1F-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "FVI8IQG1F-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_FVI8IQG1F: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [01:10<01:46,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_FYXD9U983: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [01:19<01:59,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP3VC3W6C-7e227290-9936-483e-90de-949c80f984d7: patch_number=116 have been skipped.\n",
      "GP3VC3W6C-7e227290-9936-483e-90de-949c80f984d7: patch_number=117 have been skipped.\n",
      "GP3VC3W6C-7e227290-9936-483e-90de-949c80f984d7: patch_number=118 have been skipped.\n",
      "GP3VC3W6C-7e227290-9936-483e-90de-949c80f984d7: patch_number=119 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_GP3VC3W6C: n=116 patches have been saved, original size=(9896, 2620, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [01:25<01:49,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GU8YLRNEP-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "GU8YLRNEP-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "GU8YLRNEP-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_GU8YLRNEP: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [01:30<01:38,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_GWWIAV2VV: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [01:36<01:32,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J926FMUHU-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "J926FMUHU-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "J926FMUHU-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_J926FMUHU: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [01:42<01:25,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KQNQBAHEH-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "KQNQBAHEH-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "KQNQBAHEH-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_KQNQBAHEH: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [01:47<01:15,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_KR4TV4KAT: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [01:52<01:06,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1UED3JAF-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "M1UED3JAF-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "M1UED3JAF-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_M1UED3JAF: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [01:58<00:59,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_N7W26YIGK: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [02:02<00:52,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_N9WCYFDNV: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [02:09<00:51,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRI4TEP79-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "NRI4TEP79-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "NRI4TEP79-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_NRI4TEP79: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [02:16<00:47,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P1VA9VFEP-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "P1VA9VFEP-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "P1VA9VFEP-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_P1VA9VFEP: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [02:21<00:39,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_Q9ALKK7XZ: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [02:26<00:33,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_UI4P3E22W: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [02:32<00:28,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VQY9V2E1Q-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "VQY9V2E1Q-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "VQY9V2E1Q-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_VQY9V2E1Q: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [02:38<00:23,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VWYBY2FQY-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "VWYBY2FQY-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "VWYBY2FQY-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_VWYBY2FQY: n=77 patches have been saved, original size=(7980, 2116, 3).\n",
      "W7AADUXTW-b7add13d-8802-46cb-8d3f-cb1ce7daf029: patch_number=0 have been skipped.\n",
      "W7AADUXTW-b7add13d-8802-46cb-8d3f-cb1ce7daf029: patch_number=8 have been skipped.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [02:46<00:18,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W7AADUXTW-b7add13d-8802-46cb-8d3f-cb1ce7daf029: patch_number=81 have been skipped.\n",
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_W7AADUXTW: n=87 patches have been saved, original size=(4708, 4541, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [02:51<00:11,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_YLTE7FRJW: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [02:56<00:05,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7add13d-8802-46cb-8d3f-cb1ce7daf029_ZVDH6FY93: n=64 patches have been saved, original size=(3999, 4016, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [03:02<00:00,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZVFAN11AI-7e227290-9936-483e-90de-949c80f984d7: patch_number=77 have been skipped.\n",
      "ZVFAN11AI-7e227290-9936-483e-90de-949c80f984d7: patch_number=78 have been skipped.\n",
      "ZVFAN11AI-7e227290-9936-483e-90de-949c80f984d7: patch_number=79 have been skipped.\n",
      "7e227290-9936-483e-90de-949c80f984d7_ZVFAN11AI: n=77 patches have been saved, original size=(7980, 2116, 3).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = creator.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "ns9fK7qLkYoB"
   },
   "outputs": [],
   "source": [
    "df.to_csv('anna/image_data_512.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "TU9ZIo3ZlnRm",
    "outputId": "61e5752e-e807-4ff1-b0a8-d196887c1930"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_code</th>\n",
       "      <th>field_token</th>\n",
       "      <th>class</th>\n",
       "      <th>flight_date</th>\n",
       "      <th>flight_number</th>\n",
       "      <th>tile_number</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1TJZT39WQ</td>\n",
       "      <td>b7add13d-8802-46cb-8d3f-cb1ce7daf029</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>anna/final_image_data/b7add13d-8802-46cb-8d3f-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1TJZT39WQ</td>\n",
       "      <td>b7add13d-8802-46cb-8d3f-cb1ce7daf029</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>anna/final_image_data/b7add13d-8802-46cb-8d3f-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1TJZT39WQ</td>\n",
       "      <td>b7add13d-8802-46cb-8d3f-cb1ce7daf029</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>anna/final_image_data/b7add13d-8802-46cb-8d3f-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1TJZT39WQ</td>\n",
       "      <td>b7add13d-8802-46cb-8d3f-cb1ce7daf029</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>anna/final_image_data/b7add13d-8802-46cb-8d3f-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1TJZT39WQ</td>\n",
       "      <td>b7add13d-8802-46cb-8d3f-cb1ce7daf029</td>\n",
       "      <td>False</td>\n",
       "      <td>2020-07-11</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>anna/final_image_data/b7add13d-8802-46cb-8d3f-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flight_code                           field_token  class flight_date  \\\n",
       "0   1TJZT39WQ  b7add13d-8802-46cb-8d3f-cb1ce7daf029  False  2020-07-11   \n",
       "1   1TJZT39WQ  b7add13d-8802-46cb-8d3f-cb1ce7daf029  False  2020-07-11   \n",
       "2   1TJZT39WQ  b7add13d-8802-46cb-8d3f-cb1ce7daf029  False  2020-07-11   \n",
       "3   1TJZT39WQ  b7add13d-8802-46cb-8d3f-cb1ce7daf029  False  2020-07-11   \n",
       "4   1TJZT39WQ  b7add13d-8802-46cb-8d3f-cb1ce7daf029  False  2020-07-11   \n",
       "\n",
       "   flight_number  tile_number  \\\n",
       "0              9            0   \n",
       "1              9            1   \n",
       "2              9            2   \n",
       "3              9            3   \n",
       "4              9            4   \n",
       "\n",
       "                                          image_path  \n",
       "0  anna/final_image_data/b7add13d-8802-46cb-8d3f-...  \n",
       "1  anna/final_image_data/b7add13d-8802-46cb-8d3f-...  \n",
       "2  anna/final_image_data/b7add13d-8802-46cb-8d3f-...  \n",
       "3  anna/final_image_data/b7add13d-8802-46cb-8d3f-...  \n",
       "4  anna/final_image_data/b7add13d-8802-46cb-8d3f-...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_QmXVWAEsiOc",
    "outputId": "57f7c911-a7e5-4f7e-fc83-73154262debe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2256, 7)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "pviF5nfqIXNY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"anna/image_data_448.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ua9eikUG0Ce5"
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import shuffle\n",
    "# flight_codes=list(set(df.flight_code))\n",
    "# flight_codes=shuffle(flight_codes, random_state=42)\n",
    "# train_ids = flight_codes[:20]\n",
    "# val_ids = flight_codes[20:25]\n",
    "# test_ids = flight_codes[25:]\n",
    "# train_data=df[df['flight_code'].isin(train_ids)]\n",
    "# val_data=df[df['flight_code'].isin(val_ids)]\n",
    "# test_data=df[df['flight_code'].isin(test_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GCo2muh0ANS4"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_data, test_data, _, _,= train_test_split(df, df, test_size=0.15, random_state=42, shuffle=True)\n",
    "# train_data, val_data, _, _,= train_test_split(train_data, train_data, test_size=0.15, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "id": "ZdA_8PXcx8Nt"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "false_ids = list(set(df[df['class'] == False]['flight_code']))\n",
    "false_ids = shuffle(false_ids, random_state=123)\n",
    "train_ids=false_ids[:13]\n",
    "val_ids=false_ids[13:16]\n",
    "test_ids=false_ids[16:]\n",
    "true_ids = list(set(df[df['class'] == True]['flight_code']))\n",
    "true_ids = shuffle(true_ids,random_state=123)\n",
    "train_ids = train_ids + true_ids[:7]\n",
    "val_ids= val_ids+true_ids[7:9]\n",
    "test_ids=test_ids + true_ids[9:]\n",
    "train_data=df[df['flight_code'].isin(train_ids)]\n",
    "val_data=df[df['flight_code'].isin(val_ids)]\n",
    "test_data=df[df['flight_code'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "id": "xbx6asSdAB5i"
   },
   "outputs": [],
   "source": [
    "# false_ids, true_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg1-ItpQMbf3",
    "outputId": "a002558f-29ec-4269-dd28-260013e54c0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_data': 1499, 'val_data': 359, 'test_data': 398}\n"
     ]
    }
   ],
   "source": [
    "print({'train_data' : len(train_data),  \n",
    "       'val_data' :   len(val_data),\n",
    "       'test_data' : len(test_data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fglfpNpQQJWp",
    "outputId": "68f7075f-6c12-41e2-90c6-68bbd7c573e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_classes': 0.3775850567044696}\n",
      "{'val_classes': 0.39275766016713093}\n",
      "{'test_classes': 0.3542713567839196}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print({'train_classes' : train_data['class'].mean()})\n",
    "print({'val_classes' : val_data['class'].mean()})\n",
    "print({'test_classes' : test_data['class'].mean()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "id": "7q5f4Xb4VwL4"
   },
   "outputs": [],
   "source": [
    "# equal observation for  two classes\n",
    "train_data1=train_data[train_data['class']==True] \n",
    "train_data2=train_data[train_data['class']==False].iloc[:train_data1.shape[0],:]\n",
    "train_data=pd.concat((train_data1, train_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbjQ9yqHVxcg",
    "outputId": "03a42952-a5b6-4c71-d526-f2669e902d6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1132, 7)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./opt/anaconda3/lib/python3.8/site-packages (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in ./opt/anaconda3/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: torchvision in ./opt/anaconda3/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in ./opt/anaconda3/lib/python3.8/site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch==1.8.1 in ./opt/anaconda3/lib/python3.8/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in ./opt/anaconda3/lib/python3.8/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "SokUK-rrEvCq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "h25kNS7bEvGc"
   },
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w65SdETwEvLP",
    "outputId": "06665ae2-7ce2-4e4e-83cc-a0a1f48845c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "id": "78sXis7qEqCO"
   },
   "outputs": [],
   "source": [
    "class CoustomDataset:\n",
    "  def __init__(self, data, transform=None):\n",
    "    self.data=data\n",
    "    self.transform=transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitems__(self):\n",
    "    from PIL import Image\n",
    "    from matplotlib import cm\n",
    "    images= torch.zeros(len(self.data), 3, image_height, image_width)\n",
    "    labels=[]\n",
    "    for idx in range(len(self.data)):\n",
    "      \n",
    "      sample_image = Image.fromarray(read_image(self.data['image_path'].iloc[idx]))\n",
    "      sample_label = self.data['class'].iloc[idx]\n",
    "      if self.transform:\n",
    "        sample_image=self.transform(sample_image)\n",
    "      images[idx] = sample_image\n",
    "      labels.append(int(sample_label))\n",
    "    labels=np.array(labels)\n",
    "    labels=torch.tensor(labels)\n",
    "\n",
    "    return (images, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "id": "TMWDPoCVhYnw"
   },
   "outputs": [],
   "source": [
    "# mean=[]\n",
    "# std=[]\n",
    "# for i in range(3):\n",
    "#   mean.append(image[:, i, :, :].mean())\n",
    "#   std.append(image[:, i, :, :].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "id": "_DlYdg0yjTiP"
   },
   "outputs": [],
   "source": [
    "# mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "id": "p3kVYDUlEqGp"
   },
   "outputs": [],
   "source": [
    "image_height=224\n",
    "image_width=224\n",
    "train_set=CoustomDataset(train_data, transforms.Compose([\n",
    "                                                         transforms.RandomRotation(90),\n",
    "                                                         transforms.RandomHorizontalFlip(),\n",
    "                                                         transforms.Resize((224,224)),\n",
    "                                                         transforms.ToTensor(),\n",
    "                                                        #  transforms.Normalize([0.2356,0.1987,0.1784], [0.0894,0.0733,0.0642]),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "yPNw7_mesPzN"
   },
   "outputs": [],
   "source": [
    "val_set=CoustomDataset(val_data, transforms.Compose([\n",
    "                                                        #  transforms.RandomRotation(30),\n",
    "                                                        #  transforms.RandomHorizontalFlip(),\n",
    "                                                         transforms.Resize((224,224)),\n",
    "                                                         transforms.ToTensor(),\n",
    "                                                        #  transforms.Normalize([0.2356,0.1987,0.1784], [0.0894,0.0733,0.0642]),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "TfPLW9eDNlaO"
   },
   "outputs": [],
   "source": [
    "train_data = train_set.__getitems__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "myzlU5hetTPq"
   },
   "outputs": [],
   "source": [
    "val_data = val_set.__getitems__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LOvQRfRjogUA",
    "outputId": "19fcfa4f-d290-4b59-c633-e77730ed6e74"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1132, 3, 224, 224]), torch.Size([1132]))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape,  train_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DYPXLYPTjE30",
    "outputId": "de3d31cc-b201-466d-ba73-bb6ddfb1efd5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].min(), train_data[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "id": "vs_NVIvoybiW"
   },
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    img, bbox = data\n",
    "    zipped = (img, bbox)\n",
    "    return list(zipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "id": "s4d1yAx999uY"
   },
   "outputs": [],
   "source": [
    "# def my_collate(batch):\n",
    "#     data = [item[0] for item in batch]\n",
    "#     target = [item[1] for item in batch]\n",
    "#     return [data, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EWuGjJwE2mKG",
    "outputId": "e2fbc3aa-fcda-4393-a6ec-61fb6a6301f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1132, 3, 224, 224]), torch.Size([1132]))"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].shape, train_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "vN56pUEutLkM"
   },
   "outputs": [],
   "source": [
    "train_loader=DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "rlVJG4g4tmjK"
   },
   "outputs": [],
   "source": [
    "val_loader=DataLoader(val_data, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "zFHFrlcTpPhN"
   },
   "outputs": [],
   "source": [
    "batch=next(iter(train_loader))\n",
    "image, label = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8JIvGuDDq3t",
    "outputId": "e078d085-9096-4f4d-da79-bf4173dc5d7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1132]), torch.Size([1132, 3, 224, 224]))"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape, image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "CY4GmD_Rtt5t"
   },
   "outputs": [],
   "source": [
    "batch_val=next(iter(val_loader))\n",
    "label_val, image_val = batch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXE-XGfGbncp",
    "outputId": "94097e2b-74d0-4585-bbe7-c75560b10e7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([359, 3, 224, 224]), torch.Size([359]))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_val.shape, label_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "zHRPs-XgC5t3"
   },
   "outputs": [],
   "source": [
    "model=models.vgg13(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "WRot1Pg_P6xA"
   },
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Te8FQiCFzCzh",
    "outputId": "772ca367-60ef-4d2a-afe0-1962abfe9433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): ReLU(inplace=True)\n",
       "    (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (23): ReLU(inplace=True)\n",
       "    (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "qOf7LAE0N-cj"
   },
   "outputs": [],
   "source": [
    "for parameter in model.features.parameters():\n",
    "    parameter.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8YoJ942pxCF",
    "outputId": "e7e9d421-9647-4c97-d319-c20b70ce8aa2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): ReLU(inplace=True)\n",
       "  (17): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (20): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (21): ReLU(inplace=True)\n",
       "  (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (23): ReLU(inplace=True)\n",
       "  (24): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")>"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "c1d91WbzN-qH"
   },
   "outputs": [],
   "source": [
    "# from  collections import OrderedDict\n",
    "# model.classifier = nn.Sequential(OrderedDict([('fc1', nn.Linear(25058, 5000)),\n",
    "#                                               ('relu', nn.ReLU()),\n",
    "#                                               ('drop', nn.Dropout(p=0.5)),\n",
    "#                                               ('fc2', nn.Linear(5000, 512)),\n",
    "#                                               ('drop', nn.Dropout(p=0.5)),\n",
    "#                                               ('output', nn.LogSigmoid())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "QI2Ek5b7DEHW"
   },
   "outputs": [],
   "source": [
    "model.classifier=nn.Sequential(\n",
    "                              #  nn.Linear(25088,512),\n",
    "                              #  nn.ReLU(),\n",
    "                              #  nn.Dropout(p=0.5),\n",
    "                               nn.Linear(25088,2),\n",
    "                               nn.Softmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "RZTzb09cjAw3"
   },
   "outputs": [],
   "source": [
    "# criterion=nn.CrossEntropyLoss()\n",
    "# optimizer=Adam(model.classifier.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "TOKl5LRSEI1Y"
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.Adam(model.classifier.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMjy8rMymon3",
    "outputId": "e7788cbb-5c61-4725-9323-d4d499789be4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "o4TPvLh_kFbE"
   },
   "outputs": [],
   "source": [
    "def train_proccess(model, image, label, optimizer, image_val, label_val):\n",
    "  epochs=7\n",
    "  batch_size=256\n",
    "\n",
    "  for e in range(epochs):\n",
    "    model.train(True)\n",
    "    running_loss=[]\n",
    "    for i in range(int(np.ceil(len(image)/batch_size))):\n",
    "        images=image[i*batch_size:np.min([(i+1)*batch_size, len(image)])]\n",
    "        labels=label[i*batch_size:np.min([(i+1)*batch_size, len(image)])]\n",
    "        images=images.to(device)\n",
    "        labels=labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        result=model.forward(images)\n",
    "\n",
    "        loss =  torch.nn.functional.cross_entropy(result, labels)\n",
    "        print('interation' , i , 'loss', float(loss))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "    print('epoch' , e , 'loss', float(np.mean(running_loss)))\n",
    "\n",
    "    phase='val'   \n",
    "    model.eval()\n",
    "    image_val=image_val.to(device)\n",
    "    label_val=label_val.to(device)\n",
    "    result= model.forward(image_val)\n",
    "    _, preds = torch.max(result, 1) \n",
    "    val_loss =  torch.nn.functional.cross_entropy(result, label_val)\n",
    "    running_corrects=torch.sum(preds == label_val.data)\n",
    "    val_acc = running_corrects.double() / len(image_val.data)\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, val_acc, val_acc))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5UD9lDOJBG1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "MtGxTXsfj3t0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# def train_proccess(model, image, label, optimizer, image_val, label_val):\n",
    "#   epochs=7\n",
    "\n",
    "#   for e in range(epochs):\n",
    "#     model.train(True)\n",
    "#     running_loss=0\n",
    "\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     result=model.forward(image)\n",
    "\n",
    "#     loss =  torch.nn.functional.cross_entropy(result, label)\n",
    "#     print('interation' , e , 'loss', float(loss))\n",
    "\n",
    "#     loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "\n",
    "#     running_loss = loss.item()\n",
    "\n",
    "#     print('epoch' , e , 'loss', float(running_loss))\n",
    "\n",
    "#     phase='val'   \n",
    "#     model.eval()\n",
    "#     output= model.forward(image_val)\n",
    "#     _, preds = torch.max(output, 1) \n",
    "#     val_loss =  torch.nn.functional.cross_entropy(output, label_val)\n",
    "#     running_corrects=torch.sum(preds == label_val.data)\n",
    "#     val_acc = running_corrects.double() / len(image_val.data)\n",
    "#     print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, val_loss, val_acc))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoWkd6YSNu-Z",
    "outputId": "a4d93688-c508-40c9-9858-6cf7322a15cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hov.odn/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interation 0 loss 0.7211850881576538\n",
      "interation 1 loss 0.6216901540756226\n",
      "interation 2 loss 0.8154634237289429\n",
      "interation 3 loss 0.875608503818512\n",
      "interation 4 loss 0.8886748552322388\n",
      "epoch 0 loss 0.784524405002594\n",
      "val Loss: 0.3983 Acc: 0.3983\n",
      "interation 0 loss 0.499651700258255\n",
      "interation 1 loss 0.5115892887115479\n",
      "interation 2 loss 0.8039498329162598\n",
      "interation 3 loss 0.8374019265174866\n",
      "interation 4 loss 0.8363716006278992\n",
      "epoch 1 loss 0.6977928698062896\n",
      "val Loss: 0.4067 Acc: 0.4067\n",
      "interation 0 loss 0.4893967807292938\n",
      "interation 1 loss 0.5195797681808472\n",
      "interation 2 loss 0.7338190078735352\n",
      "interation 3 loss 0.7417213916778564\n",
      "interation 4 loss 0.7381330132484436\n",
      "epoch 2 loss 0.6445299923419953\n",
      "val Loss: 0.7131 Acc: 0.7131\n",
      "interation 0 loss 0.5118350982666016\n",
      "interation 1 loss 0.5555571913719177\n",
      "interation 2 loss 0.6489985585212708\n",
      "interation 3 loss 0.6369611024856567\n",
      "interation 4 loss 0.6357380151748657\n",
      "epoch 3 loss 0.5978179931640625\n",
      "val Loss: 0.7521 Acc: 0.7521\n",
      "interation 0 loss 0.5417975187301636\n",
      "interation 1 loss 0.5958815813064575\n",
      "interation 2 loss 0.5798636674880981\n",
      "interation 3 loss 0.5565550923347473\n",
      "interation 4 loss 0.5583920478820801\n",
      "epoch 4 loss 0.5664979815483093\n",
      "val Loss: 0.6880 Acc: 0.6880\n",
      "interation 0 loss 0.5583173632621765\n",
      "interation 1 loss 0.6198233366012573\n",
      "interation 2 loss 0.535735011100769\n",
      "interation 3 loss 0.5083661675453186\n",
      "interation 4 loss 0.5115691423416138\n",
      "epoch 5 loss 0.546762204170227\n",
      "val Loss: 0.6685 Acc: 0.6685\n",
      "interation 0 loss 0.5507714152336121\n",
      "interation 1 loss 0.6195688843727112\n",
      "interation 2 loss 0.509123682975769\n",
      "interation 3 loss 0.4837206304073334\n",
      "interation 4 loss 0.4865545332431793\n",
      "epoch 6 loss 0.529947829246521\n",
      "val Loss: 0.6685 Acc: 0.6685\n"
     ]
    }
   ],
   "source": [
    "train_proccess(model, image, label, optimizer, image_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "N8WScdKA9dCp"
   },
   "outputs": [],
   "source": [
    "test_set=CoustomDataset(test_data, transforms.Compose([\n",
    "                                                         transforms.Resize((224,224)),\n",
    "                                                         transforms.ToTensor()                                                       \n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "pSCag8wc-gMJ"
   },
   "outputs": [],
   "source": [
    "test_data = test_set.__getitems__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "PRUBuUVZAqOz"
   },
   "outputs": [],
   "source": [
    "test_loader=DataLoader(test_data, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "RBda6L3OCMMC"
   },
   "outputs": [],
   "source": [
    "batch=next(iter(test_loader))\n",
    "image_test, label_test = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmkESNdvB07-",
    "outputId": "11d758e9-fc3a-42fb-9250-42f8e6f77258"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([398, 3, 224, 224]), torch.Size([398]))"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_test.shape, label_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "EytRWFj7-mDW"
   },
   "outputs": [],
   "source": [
    "def model_accuracy(model, image, label):\n",
    "  model.eval()\n",
    "  output= model.forward(image)\n",
    "  _, preds = torch.max(output, 1) \n",
    "  running_corrects=torch.sum(preds == label.data)\n",
    "  acc = running_corrects.double() / len(image.data)\n",
    "  print(' Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WE7DELpGAulW",
    "outputId": "07d36fa2-c650-4e4c-8fb2-7ecab3271b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.6685\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(model, image_val, label_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DbHhUXXrCdjN",
    "outputId": "ed8ae205-0f35-468c-b10f-b88c39b826a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 0.6508\n"
     ]
    }
   ],
   "source": [
    "model_accuracy(model, image_test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "luq4zm_nQfXC",
    "outputId": "2afff27f-ba9d-4187-8fa0-2241282983ec"
   },
   "outputs": [],
   "source": [
    "model_output= model.forward(image_test)\n",
    "\n",
    "_, preds = torch.max(model_output, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08P3Iv_YDKVA",
    "outputId": "9b1db4f4-0fa0-4a99-f0ae-c3ed4ac414ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8FSyAU9De_0",
    "outputId": "3f28ed2d-cd7c-47fd-9aca-980bd2f041e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "XU2k8r-ODkNT"
   },
   "outputs": [],
   "source": [
    "y_pred=preds.numpy()\n",
    "y_true=label_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4P4MbUCAD3Y_",
    "outputId": "529121ad-2234-4dce-cad4-189625c47a4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398,), (398,))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "aYyzZ6r0_8Zp"
   },
   "outputs": [],
   "source": [
    "def plot_conf_matrix(cm, classes, normalize=True, title='confusion_matrix'):\n",
    "  plt.imshow(cm , interpolation='nearest')\n",
    "  plt.title(title)\n",
    "  plt.colorbar()\n",
    "  tick_marks=np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes, rotation=25)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "  if normalize:\n",
    "    cm=cm.astype('float32') / cm.sum(axis=1)[:, np.newaxis]\n",
    "  thresh = cm.max() / 2\n",
    "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, np.round(cm[i,j], 2))\n",
    "  plt.tight_layout()\n",
    "  plt.ylabel(\"True label\")\n",
    "  plt.xlabel(\"Predicted label\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "id": "Gqebf5yYCA-g",
    "outputId": "17268f67-6418-4cdb-d256-9bf36d1ace5a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(y_true, y_pred)\n",
    "cm_class=['False', \"True\"]\n",
    "plot_conf_matrix(cm, cm_class, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "J6rrz2Kf_Kg7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, roc_auc_score, accuracy_score, roc_curve, auc\n",
    "def evaluation_matrics(y_test, predictions):\n",
    "  print({'accuracy for False class': accuracy_score(y_test[y_test==False], predictions[y_test==False].reshape(-1))})\n",
    "  print({'accuracy for True class': accuracy_score(y_test[y_test==True], predictions[y_test==True].reshape(-1))})\n",
    "  print({'accuracy ': accuracy_score(y_test, predictions.reshape(-1))})\n",
    "  # precision \n",
    "  precision = precision_score(y_test, predictions)\n",
    "  print('Precision: %f' % precision)\n",
    "  # recall:\n",
    "  recall = recall_score(y_test, predictions)\n",
    "  print('Recall: %f' % recall)\n",
    "  # f1 scorw:\n",
    "  f1 = f1_score(y_test, predictions)\n",
    "  print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwSifoDLpLML",
    "outputId": "9d52cc61-509d-49db-ecfe-998ac77af350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy for False class': 0.8093385214007782}\n",
      "{'accuracy for True class': 0.3617021276595745}\n",
      "{'accuracy ': 0.6507537688442211}\n",
      "Precision: 0.510000\n",
      "Recall: 0.361702\n",
      "F1 score: 0.423237\n"
     ]
    }
   ],
   "source": [
    "evaluation_matrics(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PLqvIjEnqODo",
    "outputId": "81720e74-02e0-42e5-e1e7-00ad880917e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.81      0.75       257\n",
      "           1       0.51      0.36      0.42       141\n",
      "\n",
      "    accuracy                           0.65       398\n",
      "   macro avg       0.60      0.59      0.59       398\n",
      "weighted avg       0.63      0.65      0.63       398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGSZfIwaLxfn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task_(2)_(1) (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
